{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple coupled linear regression\n",
    "*Dario Garcia*\n",
    "\n",
    "## Problem setting\n",
    "We have two regression problems on the same input space $\\mathbb{R}^k$ which we believe that are very similar. Under linear regression, this assumption translates into similar weight vectors $w_0$ and $w_1$ (i.e. small euclidean distance). How can we exploit this information?\n",
    "\n",
    "## A simple solution\n",
    "We can think of learning simultaneously both models while enforcing similarity between them by using an extra additive regularization term that can be interpreted as a simple case of intrinsic Laplacian regularization on an augmented weight space $\\mathcal{W}^* = \\mathcal{W} \\times \\mathcal{W}$ which is the cartesian product of the original weight space with itself. The objective function to minimize is then\n",
    "$$\n",
    "J(\\mathbf{w}) = \\sum_{i=1}^N l(w^t\\mathbf{x}_i, y_i) + \\lambda ||\\mathbf{w}||^2 + \\lambda_I \\mathbf{w}^t L \\mathbf{w}\n",
    "$$\n",
    "(note that if we let $\\lambda_I=0$ we are virtually learning both models independently)\n",
    "\n",
    "In our case the affinity matrix $A$ for the weights Laplacian matrix has a very simple structure, since we are only coupling analogue weights for the two vectors, i.e. $w_i^0$ and $w_i^1$, so that $A_{ij} = 1$ if $i==j$ or $i+k=j$ or $i-k=j$, $0$ otherwise. Then we can get the Laplacian matrix as $L = D - A$ where $D$ is the degree matrix. We could also absorve the standard regularization term by adding $\\lambda \\mathbf{I}$ to $L$.\n",
    "\n",
    "## Numerical optimization\n",
    "This a nice problem for doing Stochastic Gradient Descent. By doing some basic matrix algebra (which I have probably done wrong ...) we end up with a simple update rule\n",
    "$$\n",
    "w_{n+1} \\leftarrow w_n - \\sigma e_n x_n + \\lambda w_n + \\lambda_I (w_n - w^c_n) (1-2c_n)\n",
    "$$\n",
    "where $e_n$ represents the prediction error at step $n$ and by $w^c$ I mean the \"complementary\" weight vector (i.e. if we are processing a sample from task 0, $w = w_0$ and $w^c=w_1$ and the other way around) and $c_n$ is the model which is active for sample $n$.\n",
    "\n",
    "## So let's try this stuff in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "K = 20 # Number of dimensions\n",
    "N = 1000 # Number of points\n",
    "s = 0.1 # Std of the 'noise' between weights\n",
    "n_s = 4 # Std of the label noise\n",
    "\n",
    "# Define the coupled weight vectors\n",
    "w_0 = np.random.randn(K)\n",
    "w_1 = w_0 + np.random.randn(K)*s\n",
    "\n",
    "c = np.random.randint(0,2,100)\n",
    "\n",
    "# Some input data\n",
    "X = np.random.randn(K,N)\n",
    "c = np.concatenate((np.zeros(N/2), np.ones(N/2)))\n",
    "y_0 = X[:, :N/2].T.dot(w_0) + np.random.randn(N/2) * n_s\n",
    "y_1 = X[:, N/2:].T.dot(w_1) + np.random.randn(N/2) * n_s\n",
    "y =  np.concatenate((y_0, y_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating w_0\n",
      "After iteration 0: 18857.137613\n",
      "After iteration 10: 7984.373495\n",
      "After iteration 20: 7873.438421\n",
      "After iteration 30: 7867.739884\n",
      "After iteration 40: 7867.223297\n",
      "After iteration 50: 7867.165196\n",
      "After iteration 60: 7867.157866\n",
      "After iteration 70: 7867.156873\n",
      "After iteration 80: 7867.156732\n",
      "After iteration 90: 7867.156711\n",
      "Error norm (w_1): 0.690\n",
      "Estimating w_1\n",
      "After iteration 0: 18384.608503\n",
      "After iteration 10: 7567.326397\n",
      "After iteration 20: 7464.710901\n",
      "After iteration 30: 7459.598971\n",
      "After iteration 40: 7459.132193\n",
      "After iteration 50: 7459.079119\n",
      "After iteration 60: 7459.072320\n",
      "After iteration 70: 7459.071379\n",
      "After iteration 80: 7459.071242\n",
      "After iteration 90: 7459.071221\n",
      "Error norm (w_1): 0.707\n"
     ]
    }
   ],
   "source": [
    "# The gradient for standard LS linear regression\n",
    "def basic_gradient(x, w, y, reg):\n",
    "    pred = x.dot(w)\n",
    "    err = pred - y\n",
    "    grad = err * x + reg * w\n",
    "    return (grad, err)\n",
    "\n",
    "# Learning parameters\n",
    "reg = 0.05\n",
    "learning_rate = 0.0005\n",
    "num_iter = 100\n",
    "coupling_reg = 0.1\n",
    "\n",
    "# Let's estimate independently w_0 and w_1\n",
    "# BTW, this is the most computationally inefficient way of doing so\n",
    "print 'Estimating w_0'\n",
    "w = np.zeros(K)\n",
    "for iter in range(num_iter):\n",
    "    cum_err = 0\n",
    "    for i in range(len(y)/2):\n",
    "        grad, err = basic_gradient(X[:,i], w, y[i], reg)\n",
    "        w += -learning_rate * grad\n",
    "        cum_err += err**2\n",
    "    if (iter % 10) == 0:\n",
    "        print 'After iteration %i: %f' % (iter, cum_err)\n",
    "print 'Error norm (w_1): %.3f' % np.linalg.norm(w-w_0)\n",
    "\n",
    "print 'Estimating w_1'\n",
    "w = np.zeros(K)\n",
    "for iter in range(num_iter):\n",
    "    cum_err = 0\n",
    "    for i in range(len(y)/2,len(y)):\n",
    "        grad, err = basic_gradient(X[:,i], w, y[i], reg)\n",
    "        w += -learning_rate * grad\n",
    "        cum_err += err**2\n",
    "    if (iter % 10) == 0:\n",
    "        print 'After iteration %i: %f' % (iter, cum_err)\n",
    "print 'Error norm (w_1): %.3f' % np.linalg.norm(w-w_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After iteration 0: 37216.741788\n",
      "After iteration 10: 15555.272857\n",
      "After iteration 20: 15349.016029\n",
      "After iteration 30: 15331.675003\n",
      "After iteration 40: 15328.353843\n",
      "After iteration 50: 15328.918811\n",
      "After iteration 60: 15319.686766\n",
      "After iteration 70: 15331.928695\n",
      "After iteration 80: 15332.870177\n",
      "After iteration 90: 15323.133325\n",
      "Error norm (w_0): 0.646\n",
      "Error norm (w_1): 0.670\n",
      "[-1.91906663 -1.51560845 -1.75426357 -1.20230718  0.70756204 -1.63598941\n",
      " -0.29701526  0.0248109  -1.27385293 -0.12348197  0.16669     0.07104064\n",
      " -0.80939521 -1.43985274  0.96473721  0.08348464  0.66842463 -0.4627149\n",
      "  0.64865604 -1.91297988 -1.84950529 -1.42873853 -1.65912575 -0.92191313\n",
      "  0.72207666 -1.49917374 -0.3466662   0.21897691 -1.21587803 -0.01291521\n",
      "  0.13998422 -0.02644362 -0.56341966 -1.40674759  1.34143764  0.05871314\n",
      "  0.75409599 -0.47492039  0.74251632 -2.18754426]\n",
      "[-1.93760735 -1.61733652 -1.63250946 -1.11788623  0.50904929 -1.74010546\n",
      " -0.23607633 -0.00250753 -1.22132978 -0.0606537  -0.23752683 -0.07247092\n",
      " -0.70712302 -1.53273703  1.2209774   0.0999371   0.73566583 -0.44639105\n",
      "  0.86936346 -1.94573634]\n",
      "[-1.93139956 -1.56263671 -1.57609282 -1.05658253  0.68032255 -1.80793218\n",
      " -0.28428469 -0.08734009 -1.2054697   0.05029416 -0.10992546 -0.03264956\n",
      " -0.680437   -1.42354419  1.40074827 -0.02888181  0.6946628  -0.48136618\n",
      "  0.84339305 -1.87231861]\n"
     ]
    }
   ],
   "source": [
    "# The gradient for our coupled problem, in a very explicit form\n",
    "def coupled_gradient(x, c, w, y, reg, coupling_reg):\n",
    "    # Note that we are not implementing exactly the formulation presented above,\n",
    "    # as the L2 norms of the individual weight vectors are decoupled, which makes\n",
    "    # for sparser and more sensible gradient updates\n",
    "    k = len(w)\n",
    "    if c==0:\n",
    "        w_seg = w[:k/2]\n",
    "    else:\n",
    "        w_seg = w[k/2:]\n",
    "        \n",
    "    pred = x.dot(w_seg)\n",
    "    \n",
    "    coupling = (w[:k/2] - w[k/2:]) * (1 - 2*c)\n",
    "    err = pred - y\n",
    "    \n",
    "    error_grad = np.zeros(k)\n",
    "    reg_term = np.zeros(k)\n",
    "    coupling_term = np.zeros(k)\n",
    "    \n",
    "    if c==0:\n",
    "        error_grad[:k/2] = err*x\n",
    "        reg_term[:k/2] = w_seg\n",
    "        coupling_term[:k/2] = coupling\n",
    "    else:\n",
    "        error_grad[k/2:] = err*x\n",
    "        reg_term[k/2:] = w_seg\n",
    "        coupling_term[k/2:] = coupling\n",
    "        \n",
    "    grad = error_grad + reg * reg_term + coupling_reg * coupling_term\n",
    "    \n",
    "    return (grad, err)\n",
    "\n",
    "# Now let's learn our shiny coupled model\n",
    "w = np.zeros(2*K)\n",
    "for iter in range(num_iter):\n",
    "    cum_err = 0\n",
    "    idx = np.random.permutation(len(y))\n",
    "    for i in idx:\n",
    "        try:\n",
    "            grad, err = coupled_gradient(X[:,i], c[i], w, y[i], reg, coupling_reg)\n",
    "        except:\n",
    "            print i\n",
    "            print X[:,i]\n",
    "            print c[i]\n",
    "            \n",
    "        w += -learning_rate * grad\n",
    "        cum_err += err**2\n",
    "\n",
    "    if (iter % 10) == 0:\n",
    "        print 'After iteration %i: %f' % (iter, cum_err)\n",
    "\n",
    "print 'Error norm (w_0): %.3f' % np.linalg.norm(w[:K]-w_0)\n",
    "print 'Error norm (w_1): %.3f' % np.linalg.norm(w[K:]-w_1)\n",
    "\n",
    "print w\n",
    "print w_0\n",
    "print w_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
